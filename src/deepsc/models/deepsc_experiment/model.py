import torch
import torch.nn as nn
import torch.nn.functional as F

# Flash Attention v2 support
try:
    from torch.nn.functional import scaled_dot_product_attention

    FLASH_ATTENTION_AVAILABLE = True
except ImportError:
    FLASH_ATTENTION_AVAILABLE = False
    print("Warning: Flash Attention not available, falling back to standard attention")


# TODOï¼šembedding çš„è¡Œæ•°ä¸åªæ˜¯num_genesï¼Œè€Œæ˜¯num_genes+1ï¼Œå› ä¸ºè¿˜æœ‰<cls> token
class GeneEmbedding(nn.Module):
    """
    Gene Embeddingåˆ†æ”¯ï¼šä¸“æ³¨äºæ•æ‰åŸºå› çš„è¯­ä¹‰è¡¨ç¤º

    å­¦ä¹ åŸºå› çš„è¯­ä¹‰è¡¨ç¤ºï¼ŒåŒ…æ‹¬ï¼š
    - åŠŸèƒ½ç›¸ä¼¼æ€§ï¼šåŠŸèƒ½ç›¸å…³çš„åŸºå› åœ¨åµŒå…¥ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘
    - é€šè·¯å…³ç³»ï¼šåŒä¸€ç”Ÿç‰©å­¦é€šè·¯çš„åŸºå› å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤º
    - è°ƒæ§å…³ç³»ï¼šè½¬å½•å› å­ä¸å…¶é¶åŸºå› ä¹‹é—´çš„å…³ç³»
    """

    # num_genes æ˜¯åŸºå› æ•°é‡ï¼ŒåŒ…æ‹¬<cls>å’Œ<pad>
    def __init__(self, embedding_dim: int, num_genes: int):
        super(GeneEmbedding, self).__init__()
        self.embedding_dim = embedding_dim
        self.gene_embedding = nn.Embedding(
            num_embeddings=num_genes + 2, embedding_dim=embedding_dim, padding_idx=0
        )

    def forward(self, gene_ids: torch.Tensor) -> torch.Tensor:
        """
        Args:
            gene_ids: åŸºå› IDåºåˆ— G = [g_1, g_2, ..., g_g], shape: (batch_size, g)

        Returns:
            gene_embeddings: åŸºå› åµŒå…¥ E_gene âˆˆ R^{gÃ—d}, shape: (batch_size, g, d)
        """
        return self.gene_embedding(gene_ids)


# éœ€ä¿®æ”¹ï¼Œæœ‰é—®é¢˜ï¼š å½’ä¸€åŒ–å’Œåˆ†ç®±åº”è¯¥æ”¾åˆ°data_collatoré‡Œé¢ï¼Œè¿™é‡Œåªåšembedding
# å…¶æ¬¡ï¼šåœ¨data_collatoré‡Œé¢è¿˜è¦åšå¥½trunctuationå’Œpaddingä»¥åŠmask.
class ExpressionEmbedding(nn.Module):
    """
    Expression Embeddingåˆ†æ”¯ï¼šä¸“æ³¨äºæ•æ‰è¡¨è¾¾é‡çš„æ•°å€¼ç‰¹å¾å’Œä¸Šä¸‹æ–‡ä¾èµ–

    è€ƒè™‘åˆ°scRNA-seqæ•°æ®çš„ç‰¹ç‚¹ï¼Œè®¾è®¡åˆ†å±‚ç¼–ç ç­–ç•¥ï¼š
    1. è¡¨è¾¾é‡å½’ä¸€åŒ–ä¸ç¦»æ•£åŒ–
    2. åˆ†å±‚è¡¨è¾¾åµŒå…¥
    """

    # num_bins æ˜¯binæ•°é‡ï¼ŒåŒ…æ‹¬<cls>å’Œ<pad>ä»¥åŠ<mask>
    def __init__(self, embedding_dim: int, num_bins: int = 50, alpha: float = 0.3):
        """
        Args:
            embedding_dim: åµŒå…¥ç»´åº¦ d
            num_bins: ç¦»æ•£åŒ–çš„binæ•°é‡ N
            alpha: å¹³è¡¡ç¦»æ•£å’Œè¿ç»­ç‰¹å¾çš„æƒé‡å‚æ•°
        """
        super(ExpressionEmbedding, self).__init__()
        self.embedding_dim = embedding_dim
        self.num_bins = num_bins
        # alpbaæ˜¯å¦å¯ä»¥ä½œä¸ºå¯å­¦ä¹ çš„å‚æ•°
        self.alpha = alpha
        # ç¦»æ•£è¡¨è¾¾æ°´å¹³çš„åµŒå…¥çŸ©é˜µ W_bin âˆˆ R^{dÃ—N}
        self.bin_embedding = nn.Embedding(
            num_embeddings=num_bins + 3, embedding_dim=embedding_dim, padding_idx=0
        )
        # è¿ç»­å€¼çš„æŠ•å½±å‘é‡ v_cont âˆˆ R^d
        self.continuous_projection = nn.Linear(1, embedding_dim, bias=True)

        # åˆå§‹åŒ–æƒé‡
        nn.init.xavier_uniform_(self.bin_embedding.weight)
        nn.init.xavier_uniform_(self.continuous_projection.weight)
        nn.init.zeros_(self.continuous_projection.bias)

    def forward(
        self, discrete_expression: torch.Tensor, normalized_expr: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            expression: è¡¨è¾¾é‡å‘é‡ x = [x_1, x_2, ..., x_g], shape: (batch_size, g)

        Returns:
            expr_embeddings: è¡¨è¾¾é‡åµŒå…¥ E_expr âˆˆ R^{gÃ—d}, shape: (batch_size, g, d)
        """

        discrete_embeddings = self.bin_embedding(discrete_expression)
        continuous_component = self.continuous_projection(normalized_expr.unsqueeze(-1))

        expr_embeddings = discrete_embeddings + continuous_component

        return expr_embeddings


class FlashAttentionLayer(nn.Module):
    """
    ç»Ÿä¸€çš„ Flash Attention v2 æ³¨æ„åŠ›å±‚

    æ¥å— Q, K, V çš„è¾“å…¥åµŒå…¥,è®¡ç®—å¤šå¤´æ³¨æ„åŠ›ã€‚

    Args:
        d: åµŒå…¥ç»´åº¦
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        attn_dropout: æ³¨æ„åŠ›dropoutç‡
    """

    def __init__(self, d, num_heads, attn_dropout=0.1):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = d // num_heads
        self.d = d

        # Q, K, V æŠ•å½±çŸ©é˜µ
        self.W_Q = nn.Linear(d, d)
        self.W_K = nn.Linear(d, d)
        self.W_V = nn.Linear(d, d)

        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(d, d)

        # Dropout
        self.dropout = nn.Dropout(attn_dropout)

        # ç¼©æ”¾å› å­
        self.scale = self.head_dim**-0.5

    def forward(self, Q_emb, K_emb=None, V_emb=None):
        """
        å‰å‘ä¼ æ’­

        Args:
            Q_emb: QueryåµŒå…¥, shape: (batch_size, seq_len, d)
            K_emb: KeyåµŒå…¥, shape: (batch_size, seq_len, d)
                   å¦‚æœä¸ºNone,ä½¿ç”¨Q_emb (self-attention)
            V_emb: ValueåµŒå…¥, shape: (batch_size, seq_len, d)
                   å¦‚æœä¸ºNone,ä½¿ç”¨K_emb

        Returns:
            output: æ³¨æ„åŠ›è¾“å‡º, shape: (batch_size, seq_len, d)

        ä½¿ç”¨ç¤ºä¾‹:
            # Self-attention: Q = K = V
            out = layer(x)

            # Cross-attention: Q != K = V
            out = layer(query, key_value)

            # å®Œå…¨è‡ªå®šä¹‰: Q, K, V éƒ½ä¸åŒ
            out = layer(q, k, v)
        """
        # é»˜è®¤å€¼å¤„ç†
        if K_emb is None:
            K_emb = Q_emb
        if V_emb is None:
            V_emb = K_emb

        batch_size, seq_len, _ = Q_emb.shape

        # è®¡ç®— Q, K, V æŠ•å½±
        Q = self.W_Q(Q_emb).view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = self.W_K(K_emb).view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = self.W_V(V_emb).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # è½¬ç½®ä»¥ä¾¿è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        K = K.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        V = V.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)

        # ä½¿ç”¨ Flash Attention v2
        if FLASH_ATTENTION_AVAILABLE:

            output = scaled_dot_product_attention(
                Q,
                K,
                V,
                attn_mask=None,
                dropout_p=self.dropout.p if self.training else 0.0,
                is_causal=False,
            )
        else:
            scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale

            # åº”ç”¨ softmax
            attn_weights = F.softmax(scores, dim=-1)
            attn_weights = self.dropout(attn_weights)
            A_bar = attn_weights

            # è®¡ç®—è¾“å‡º
            output = torch.matmul(A_bar, V)

        # è½¬ç½®å¹¶é‡å¡‘
        output = output.transpose(
            1, 2
        ).contiguous()  # (batch_size, seq_len, num_heads, head_dim)
        output = output.view(batch_size, seq_len, self.d)  # (batch_size, seq_len, d)

        # è¾“å‡ºæŠ•å½±
        output = self.out_proj(output)

        return output


class FeedForward(nn.Module):
    def __init__(self, d, hidden_dim=None, dropout=0.1, num_layers=2):
        super().__init__()
        hidden_dim = hidden_dim or d * 4
        self.d = d
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # åˆ›å»ºæ¯ä¸€å±‚çš„æ¨¡å—
        self.layers = nn.ModuleList()

        # ç¬¬ä¸€å±‚ï¼šd -> hidden_dim
        first_layer = nn.Sequential(
            nn.Linear(d, hidden_dim), nn.GELU(), nn.Dropout(dropout)
        )
        self.layers.append(first_layer)

        # ä¸­é—´å±‚ï¼šhidden_dim -> hidden_dim
        for _ in range(num_layers - 2):
            middle_layer = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout)
            )
            self.layers.append(middle_layer)

        # æœ€åä¸€å±‚ï¼šhidden_dim -> d
        last_layer = nn.Sequential(nn.Linear(hidden_dim, d), nn.Dropout(dropout))
        self.layers.append(last_layer)

    def forward(self, x):
        if self.num_layers == 2:
            # åªæœ‰ä¸¤å±‚çš„æƒ…å†µï¼šd -> hidden_dim -> d
            h = self.layers[0](x)  # d -> hidden_dim
            h = self.layers[1](h)  # hidden_dim -> d
            return x + h  # æ®‹å·®è¿æ¥

        else:
            # å¤šå±‚çš„æƒ…å†µ
            # ç¬¬ä¸€å±‚
            h = self.layers[0](x)  # d -> hidden_dim

            # ä¸­é—´å±‚ï¼Œæ¯å±‚éƒ½æœ‰æ®‹å·®è¿æ¥
            for i in range(1, self.num_layers - 1):
                residual = h
                h = self.layers[i](h)  # hidden_dim -> hidden_dim
                h = h + residual  # æ®‹å·®è¿æ¥

            # æœ€åä¸€å±‚
            h = self.layers[-1](h)  # hidden_dim -> d
            return x + h  # æœ€ç»ˆæ®‹å·®è¿æ¥


class MoERegressor(nn.Module):
    """
    Mixture of Experts (MoE) å›å½’å™¨

    åŒ…å«ä¸€ä¸ªgateç½‘ç»œå’Œä¸‰ä¸ªexpertç½‘ç»œï¼š
    - Expert 1: ä¸“é—¨å¤„ç†å°å€¼ï¼ˆä½è¡¨è¾¾é‡ï¼‰
    - Expert 2: ä¸“é—¨å¤„ç†ä¸­ç­‰å€¼ï¼ˆä¸­ç­‰è¡¨è¾¾é‡ï¼‰
    - Expert 3: ä¸“é—¨å¤„ç†å¤§å€¼ï¼ˆé«˜è¡¨è¾¾é‡ï¼‰

    æ‰€æœ‰expertä½¿ç”¨ç›¸åŒçš„ç½‘ç»œç»“æ„ï¼Œé€šè¿‡gateç½‘ç»œå­¦ä¹ ä¸“é—¨åŒ–
    """

    def __init__(
        self, embedding_dim, dropout=0.1, number_of_experts=3, gate_temperature=1.0
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.num_experts = number_of_experts
        self.gate_temperature = gate_temperature

        # Gateç½‘ç»œï¼šå†³å®šæ¯ä¸ªexpertçš„æƒé‡
        self.gate = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embedding_dim // 2, self.num_experts),
        )

        # åˆ›å»ºä¸‰ä¸ªç›¸åŒç»“æ„çš„expertç½‘ç»œ
        self.experts = nn.ModuleList(
            [RegressorExpert(embedding_dim, dropout) for _ in range(self.num_experts)]
        )

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

    def _initialize_weights(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç½‘ç»œçš„æƒé‡"""
        # åˆå§‹åŒ–gateç½‘ç»œ
        for m in self.gate:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

        # åˆå§‹åŒ–æ‰€æœ‰expertç½‘ç»œ
        for expert in self.experts:
            for m in expert.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, embedding_dim)
        Returns:
            output: (batch_size, seq_len)
            gate_weights: (batch_size, seq_len, num_experts)
        """
        # 1) gate logits -> softmax æƒé‡
        gate_logits = self.gate(x)  # (B, L, E)
        gate_weights = F.softmax(gate_logits / self.gate_temperature, dim=-1)

        # 2) é€ä¸ª expert å‰å‘
        expert_outputs = []
        for expert in self.experts:
            y = expert(x)  # (B, L, 1)
            expert_outputs.append(y)

        # 3) å †å  -> (B, L, E)
        expert_outputs = torch.cat(expert_outputs, dim=-1)  # (B, L, E)

        # 4) åŠ æƒæ±‚å’Œ -> (B, L)
        output = torch.sum(gate_weights * expert_outputs, dim=-1)

        return output, gate_weights


class RegressorExpert(nn.Module):
    def __init__(self, embedding_dim, dropout):
        super().__init__()
        self.fc1 = nn.Linear(embedding_dim, embedding_dim * 2)
        self.fc2 = nn.Linear(embedding_dim * 2, embedding_dim)
        self.fc3 = nn.Linear(embedding_dim, 1)
        self.dropout = nn.Dropout(dropout)
        self.gelu = nn.GELU()
        self.norm1 = nn.LayerNorm(embedding_dim)

    def forward(self, x):
        # ---- ç¬¬ä¸€å­å±‚ ----
        residual = x
        x = self.fc1(x)
        x = self.gelu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        x = x + residual  # æ®‹å·®è¿æ¥
        x = self.norm1(x)  # Post-Norm

        # ---- ç¬¬äºŒå­å±‚ ----
        # è¾“å‡ºå›å½’å€¼ï¼Œç»´åº¦ä» embedding_dim -> 1
        x = self.fc3(x)  # (B, L, 1)

        return x


import torch
import torch.nn as nn


class MLP(nn.Module):
    """
    Multi-Layer Perceptron (MLP) used as a feed-forward layer.

    Attributes:
        w1 (nn.Module): Linear layer for input-to-hidden transformation.
        w2 (nn.Module): Linear layer for hidden-to-output transformation.
        w3 (nn.Module): Additional linear layer for feature transformation.
    """

    def __init__(self, dim: int, inter_dim: int):
        """
        Initializes the MLP layer.

        Args:
            dim (int): Input and output dimensionality.
            inter_dim (int): Hidden layer dimensionality.
        """
        super().__init__()
        self.w1 = nn.Linear(dim, inter_dim)
        self.w2 = nn.Linear(inter_dim, dim)
        self.w3 = nn.Linear(dim, inter_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the MLP layer.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor after MLP computation.
        """
        return self.w2(F.silu(self.w1(x)) * self.w3(x))


class Gate(nn.Module):
    def __init__(self, moe_cfg):
        super().__init__()
        self.dim = moe_cfg.dim
        self.topk = moe_cfg.n_activated_experts
        self.score_func = moe_cfg.score_func
        self.route_scale = moe_cfg.route_scale
        self.n_routed_experts = int(moe_cfg.n_routed_experts)
        self.proj = nn.Linear(self.dim, self.n_routed_experts, bias=True)

        # ç”¨äºç»Ÿè®¡ä¸“å®¶ä½¿ç”¨æƒ…å†µ
        self.register_buffer("expert_usage_count", torch.zeros(self.n_routed_experts))
        self.register_buffer("total_tokens", torch.zeros(1))

    def forward(self, x: torch.Tensor):
        """
        Args:
            x: [B, dim]
        Returns:
            weights: [B, topk]
            indices: [B, topk]
        """
        # [B, n_experts]
        scores = self.proj(x)

        # softmax/sigmoid
        if self.score_func == "softmax":
            scores = scores.softmax(dim=-1, dtype=torch.float32)
        else:
            scores = scores.sigmoid()
        original_scores = scores

        indices = torch.topk(scores, self.topk, dim=-1)[1]  # [B, topk]
        weights = original_scores.gather(1, indices)  # [B, topk]

        # # å½’ä¸€åŒ–
        if self.score_func == "sigmoid":
            weights = weights / weights.sum(dim=-1, keepdim=True)

        weights = weights * self.route_scale

        # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨æƒ…å†µï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼‰
        if self.training:
            batch_size = x.size(0)
            # ä¿®æ­£ï¼šæ¯ä¸ªtokené€‰æ‹©topkä¸ªä¸“å®¶ï¼Œæ‰€ä»¥æ€»çš„ä¸“å®¶é€‰æ‹©æ¬¡æ•°æ˜¯ batch_size * topk
            self.total_tokens += batch_size * self.topk

            # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
            for expert_idx in range(self.n_routed_experts):
                count = (indices == expert_idx).sum().float()
                self.expert_usage_count[expert_idx] += count

        return weights.type_as(x), indices

    def get_expert_usage_stats(self):
        """
        è·å–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            dict: åŒ…å«å„ç§ç»Ÿè®¡æŒ‡æ ‡çš„å­—å…¸
        """
        if self.total_tokens == 0:
            return {
                "expert_usage_ratio": torch.zeros(self.n_routed_experts),
                "max_usage_ratio": 0.0,
                "min_usage_ratio": 0.0,
                "usage_variance": 0.0,
                "collapse_ratio": 0.0,
                "entropy": 0.0,
                "total_tokens": 0,
            }

        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨æ¯”ä¾‹
        usage_ratio = self.expert_usage_count / self.total_tokens

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        max_usage = usage_ratio.max().item()
        min_usage = usage_ratio.min().item()
        usage_variance = usage_ratio.var().item()

        # è®¡ç®—å¡Œç¼©æ¯”ä¾‹ï¼ˆæœ€å¸¸ç”¨ä¸“å®¶çš„ä½¿ç”¨æ¯”ä¾‹ï¼‰
        collapse_ratio = max_usage

        # è®¡ç®—ç†µï¼ˆè¡¡é‡ä¸“å®¶ä½¿ç”¨çš„å‡åŒ€ç¨‹åº¦ï¼‰
        # é¿å…log(0)çš„æƒ…å†µ
        epsilon = 1e-10
        usage_ratio_safe = usage_ratio + epsilon
        entropy = -(usage_ratio_safe * torch.log(usage_ratio_safe)).sum().item()

        return {
            "expert_usage_ratio": usage_ratio,
            "max_usage_ratio": max_usage,
            "min_usage_ratio": min_usage,
            "usage_variance": usage_variance,
            "collapse_ratio": collapse_ratio,
            "entropy": entropy,
            "total_tokens": self.total_tokens.item(),
        }

    def reset_usage_stats(self):
        """é‡ç½®ä½¿ç”¨ç»Ÿè®¡"""
        self.expert_usage_count.zero_()
        self.total_tokens.zero_()

    def is_collapsed(self, threshold=0.8):
        """
        åˆ¤æ–­MoEæ˜¯å¦å¡Œç¼©

        Args:
            threshold: å¡Œç¼©é˜ˆå€¼ï¼Œå¦‚æœæŸä¸ªä¸“å®¶ä½¿ç”¨æ¯”ä¾‹è¶…è¿‡æ­¤é˜ˆå€¼åˆ™è®¤ä¸ºå¡Œç¼©

        Returns:
            bool: æ˜¯å¦å¡Œç¼©
        """
        stats = self.get_expert_usage_stats()
        return stats["collapse_ratio"] > threshold


class Expert(nn.Module):
    def __init__(self, moe_cfg, p_dropout: float = 0.0, use_bias: bool = True):
        super().__init__()
        self.w1 = nn.Linear(moe_cfg.dim, moe_cfg.moe_inter_dim, bias=use_bias)
        self.w3 = nn.Linear(moe_cfg.dim, moe_cfg.moe_inter_dim, bias=use_bias)
        self.w2 = nn.Linear(moe_cfg.moe_inter_dim, moe_cfg.dim, bias=use_bias)
        self.dropout = nn.Dropout(p_dropout)

        nn.init.xavier_uniform_(self.w1.weight)
        nn.init.zeros_(self.w1.bias)
        nn.init.xavier_uniform_(self.w3.weight)
        nn.init.zeros_(self.w3.bias)
        nn.init.xavier_uniform_(self.w2.weight)
        nn.init.zeros_(self.w2.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # SwiGLU / Gated-MLP
        h = F.silu(self.w1(x)) * self.w3(x)
        h = self.dropout(h)
        out = self.w2(h)
        return out


class MoE(nn.Module):
    """
    Mixture-of-Experts (MoE) module.

    Attributes:
        dim (int): Dimensionality of input features.
        n_routed_experts (int): Total number of experts in the model.
        n_local_experts (int): Number of experts handled locally in distributed systems.
        n_activated_experts (int): Number of experts activated for each input.
        gate (nn.Module): Gating mechanism to route inputs to experts.
        experts (nn.ModuleList): List of expert modules.
        shared_experts (nn.Module): Shared experts applied to all inputs.
    """

    def __init__(
        self,
        moe_cfg,
    ):
        """
        Initializes the MoE module.

        Args:
            args (ModelArgs): Model arguments containing MoE parameters.
        """
        super().__init__()
        self.dim = moe_cfg.dim
        self.n_routed_experts = int(moe_cfg.n_routed_experts)
        self.n_activated_experts = int(moe_cfg.n_activated_experts)
        self.moe_inter_dim = int(moe_cfg.moe_inter_dim)
        self.n_shared_experts = int(moe_cfg.n_shared_experts)
        self.route_scale = float(moe_cfg.route_scale)
        self.moe_cfg = moe_cfg
        self.gate = Gate(self.moe_cfg)

        self.experts = nn.ModuleList(
            [Expert(self.moe_cfg) for i in range(self.n_routed_experts)]
        )
        self.shared_experts = MLP(self.dim, self.n_shared_experts * self.moe_inter_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the MoE module.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor after expert routing and computation.
        """
        shape = x.size()
        x = x.view(-1, self.dim)
        weights, indices = self.gate(x)
        y = torch.zeros_like(x)

        counts = torch.bincount(
            indices.flatten(), minlength=self.n_routed_experts
        ).tolist()
        for i in range(self.n_routed_experts):
            if counts[i] == 0:
                continue
            expert = self.experts[i]
            idx, top = torch.where(indices == i)
            y[idx] += expert(x[idx]) * weights[idx, top, None]
        z = self.shared_experts(x)
        return (y + z).view(shape)

    def get_expert_usage_stats(self):
        """è·å–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.gate.get_expert_usage_stats()

    def reset_usage_stats(self):
        """é‡ç½®ä½¿ç”¨ç»Ÿè®¡"""
        self.gate.reset_usage_stats()

    def is_collapsed(self, threshold=0.8):
        """åˆ¤æ–­MoEæ˜¯å¦å¡Œç¼©"""
        return self.gate.is_collapsed(threshold)


class FlashDeepSCTransformerBlock(nn.Module):
    """
    ä½¿ç”¨ Flash Attention v2 çš„Transformerå—
    """

    def __init__(
        self,
        embedding_dim,
        num_heads,
        attn_dropout,
        ffn_dropout,
        num_layers_ffn=2,
        moe_cfg=None,
        moe_layer=False,
    ):
        super().__init__()
        self.num_heads = num_heads
        self.embedding_dim = embedding_dim
        self.use_moe_ffn = bool(moe_layer)
        # ä½¿ç”¨ Flash Attention å±‚
        self.gene_attn = FlashAttentionLayer(embedding_dim, num_heads, attn_dropout)
        self.expr_attn = FlashAttentionLayer(embedding_dim, num_heads, attn_dropout)

        # Norm
        self.norm_gene1 = nn.LayerNorm(embedding_dim)
        self.norm_gene2 = nn.LayerNorm(embedding_dim)
        self.norm_expr1 = nn.LayerNorm(embedding_dim)
        self.norm_expr2 = nn.LayerNorm(embedding_dim)

        # FFN
        # self.ffn_gene = FeedForward(
        #     embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
        # )
        # self.ffn_expr = FeedForward(
        #     embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
        # )
        self.dropout = nn.Dropout(ffn_dropout)
        self.ffn_gene = (
            MoE(moe_cfg)
            if self.use_moe_ffn
            else FeedForward(
                embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
            )
        )
        self.ffn_expr = (
            MoE(moe_cfg)
            if self.use_moe_ffn
            else FeedForward(
                embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
            )
        )

    def forward(self, gene_emb, expr_emb):
        """
        Args:
            gene_emb: åŸºå› åµŒå…¥, shape: (batch_size, seq_len, embedding_dim)
            expr_emb: è¡¨è¾¾åµŒå…¥, shape: (batch_size, seq_len, embedding_dim)
        Returns:
            out_gene: æ›´æ–°åçš„åŸºå› åµŒå…¥
            out_expr: æ›´æ–°åçš„è¡¨è¾¾åµŒå…¥
        """
        # Gene self-attention
        x = self.norm_gene1(gene_emb)
        attn_gene = self.gene_attn(x)  # Self-attention: Q=K=V=x
        x = gene_emb + self.dropout(attn_gene)
        x_ln = self.norm_gene2(x)
        ffn_gene = self.ffn_gene(x_ln)
        out_gene = x + self.dropout(ffn_gene)

        # Expression cross-attention with gene
        y = self.norm_expr1(expr_emb)
        attn_expr = self.expr_attn(y, gene_emb)  # Cross-attention: Q=y, K=V=gene_emb
        y = expr_emb + self.dropout(attn_expr)
        y_ln = self.norm_expr2(y)
        ffn_expr = self.ffn_expr(y_ln)
        out_expr = y + self.dropout(ffn_expr)

        return out_gene, out_expr


class FlashDeepSCTransformerCrossAttentionBlock(nn.Module):
    """
    ä½¿ç”¨ Flash Attention v2 çš„Transformerå—
    """

    def __init__(
        self,
        embedding_dim,
        num_heads,
        attn_dropout,
        ffn_dropout,
        num_layers_ffn=2,
        moe_cfg=None,
        moe_layer=False,
    ):
        super().__init__()
        self.num_heads = num_heads
        self.embedding_dim = embedding_dim
        self.use_moe_ffn = bool(moe_layer)
        # ä½¿ç”¨ Flash Attention å±‚
        self.gene_attn = FlashAttentionLayer(embedding_dim, num_heads, attn_dropout)
        self.expr_attn = FlashAttentionLayer(embedding_dim, num_heads, attn_dropout)

        # Norm
        self.norm_gene1 = nn.LayerNorm(embedding_dim)
        self.norm_gene2 = nn.LayerNorm(embedding_dim)
        self.norm_expr1 = nn.LayerNorm(embedding_dim)
        self.norm_expr2 = nn.LayerNorm(embedding_dim)

        # FFN
        # self.ffn_gene = FeedForward(
        #     embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
        # )
        # self.ffn_expr = FeedForward(
        #     embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
        # )
        self.dropout = nn.Dropout(ffn_dropout)
        self.ffn_gene = (
            MoE(moe_cfg)
            if self.use_moe_ffn
            else FeedForward(
                embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
            )
        )
        self.ffn_expr = (
            MoE(moe_cfg)
            if self.use_moe_ffn
            else FeedForward(
                embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
            )
        )

    def forward(self, gene_emb, expr_emb):
        """
        Args:
            gene_emb: åŸºå› åµŒå…¥, shape: (batch_size, seq_len, embedding_dim)
            expr_emb: è¡¨è¾¾åµŒå…¥, shape: (batch_size, seq_len, embedding_dim)
        Returns:
            out_gene: æ›´æ–°åçš„åŸºå› åµŒå…¥
            out_expr: æ›´æ–°åçš„è¡¨è¾¾åµŒå…¥
        """
        # Gene self-attention
        x = self.norm_gene1(gene_emb)
        attn_gene = self.gene_attn(x)  # Self-attention: Q=K=V=x
        x = gene_emb + self.dropout(attn_gene)
        x_ln = self.norm_gene2(x)
        ffn_gene = self.ffn_gene(x_ln)
        out_gene = x + self.dropout(ffn_gene)

        # Expression cross-attention with gene
        y = self.norm_expr1(expr_emb)
        attn_expr = self.expr_attn(y, gene_emb)  # Cross-attention: Q=y, K=V=gene_emb
        y = expr_emb + self.dropout(attn_expr)
        y_ln = self.norm_expr2(y)
        ffn_expr = self.ffn_expr(y_ln)
        out_expr = y + self.dropout(ffn_expr)

        return out_gene, out_expr


class FlashDeepSCTransformerSelfAttentionBlock(nn.Module):
    """
    ä½¿ç”¨ Flash Attention v2 çš„Transformerå—
    """

    def __init__(
        self,
        embedding_dim,
        num_heads,
        attn_dropout,
        ffn_dropout,
        num_layers_ffn=2,
        moe_cfg=None,
        use_moe_in_layer=False,
    ):
        super().__init__()
        self.num_heads = num_heads
        self.embedding_dim = embedding_dim
        self.use_moe_in_layer = use_moe_in_layer
        self.expr_attn = FlashAttentionLayer(embedding_dim, num_heads, attn_dropout)
        self.norm_expr1 = nn.LayerNorm(embedding_dim)
        self.norm_expr2 = nn.LayerNorm(embedding_dim)

        self.ffn_expr = (
            MoE(moe_cfg)
            if self.use_moe_in_layer
            else FeedForward(
                embedding_dim, dropout=ffn_dropout, num_layers=num_layers_ffn
            )
        )
        self.dropout = nn.Dropout(ffn_dropout)

    def forward(self, embedding):
        """
        Args:
            embedding: åŸºå› æˆ–è€…è¡¨è¾¾åµŒå…¥, shape: (batch_size, seq_len, embedding_dim)
        Returns:
            out_embedding: æ›´æ–°åçš„åµŒå…¥
        """

        y = self.norm_expr1(embedding)
        attn_expr = self.expr_attn(y)  # Self-attention: Q=K=V=y
        y = embedding + self.dropout(attn_expr)
        y_ln = self.norm_expr2(y)
        ffn_expr = self.ffn_expr(y_ln)
        out_expr = y + self.dropout(ffn_expr)

        return out_expr


class DeepSC(nn.Module):
    """
    ä½¿ç”¨ Flash Attention v2 çš„DeepSCæ¨¡å‹
    å½“å‰è¿›è¡Œå››ä¸ªå®éªŒæ¯”è¾ƒï¼š
    1. ï¼šå…ˆå¯¹åŸºå› åµŒå…¥è¿›è¡ŒNå±‚self-attentionï¼Œ
            ç„¶åç”¨è¿™ä¸ªå¾—åˆ°çš„åµŒå…¥ä½œä¸ºQå’ŒKï¼Œç”¨è¡¨è¾¾åµŒå…¥ä½œä¸ºVè¿›è¡ŒNå±‚cross attentionä¹‹åï¼Œ
            ç„¶ååœ¨è¿›è¡ŒMå±‚expressionçš„self-attention
    2. åŸºå› åµŒå…¥å’Œè¡¨è¾¾åµŒå…¥è¿›è¡ŒåŒæµçš„cross attention
    ç”¨ä»¥ä¸Šç»“æœè¾ƒå¥½çš„ç»“æ„ä½œä¸ºä¸‹é¢ä¸¤ä¸ªå®éªŒçš„åŸºç¡€ï¼š
    3. KVæ¥è‡ªåŸºå› åµŒå…¥ï¼ŒQæ¥è‡ªè¡¨è¾¾åµŒå…¥çš„å•æµattention
    4. Qæ¥è‡ªåŸºå› åµŒå…¥ï¼ŒKVæ¥è‡ªè¡¨è¾¾åµŒå…¥çš„å•æµattention

    """

    def __init__(
        self,
        embedding_dim,
        num_genes,
        num_layers=4,
        num_heads=8,
        attn_dropout=0.1,
        ffn_dropout=0.1,
        num_bins=8,
        alpha=0.1,
        mask_layer_start=None,
        enable_l0=True,
        enable_mse=True,
        enable_ce=True,
        num_layers_ffn=2,
        use_moe_regressor=True,
        number_of_experts=3,
        gene_embedding_participate_til_layer=3,
        moe=None,
        experiment=1,
    ):
        super().__init__()
        self.gene_embedding_participate_til_layer = gene_embedding_participate_til_layer
        self.gene_embedding = GeneEmbedding(embedding_dim, num_genes)
        self.expr_embedding = ExpressionEmbedding(
            embedding_dim, num_bins=num_bins, alpha=alpha
        )
        num_layers_expr = num_layers - gene_embedding_participate_til_layer
        self.num_heads = num_heads
        if experiment == 2:
            self.layers = nn.ModuleList()
            for i in range(gene_embedding_participate_til_layer):
                self.layers.append(
                    FlashDeepSCTransformerBlock(
                        embedding_dim,
                        num_heads,
                        attn_dropout,
                        ffn_dropout,
                        num_layers_ffn,
                        moe,
                    )
                )
            self.expression_layers = nn.ModuleList()
            for i in range(num_layers_expr):
                moe_layer = i >= (num_layers_expr - moe.n_moe_layers)
                self.expression_layers.append(
                    FlashDeepSCTransformerSelfAttentionBlock(
                        embedding_dim,
                        num_heads,
                        attn_dropout,
                        ffn_dropout,
                        num_layers_ffn,
                        moe,
                        use_moe_in_layer=moe_layer and moe.use_moe_ffn,
                    )
                )
        elif experiment == 1:
            self.gene_self_attention_layers = nn.ModuleList()
            for i in range(gene_embedding_participate_til_layer):
                self.gene_self_attention_layers.append(
                    FlashDeepSCTransformerSelfAttentionBlock(
                        embedding_dim,
                        num_heads,
                        attn_dropout,
                        ffn_dropout,
                        num_layers_ffn,
                        moe,
                        use_moe_in_layer=moe_layer and moe.use_moe_ffn,
                    )
                )
            self.cross_attention_layers = nn.ModuleList()
            for i in range(gene_embedding_participate_til_layer):
                self.cross_attention_layers.append(
                    FlashDeepSCTransformerBlock(
                        embedding_dim,
                        num_heads,
                        attn_dropout,
                        ffn_dropout,
                        num_layers_ffn,
                        moe,
                    )
                )

            self.expression_layers = nn.ModuleList()
            for i in range(num_layers_expr):
                moe_layer = i >= (num_layers_expr - moe.n_moe_layers)
                self.expression_layers.append(
                    FlashDeepSCTransformerSelfAttentionBlock(
                        embedding_dim,
                        num_heads,
                        attn_dropout,
                        ffn_dropout,
                        num_layers_ffn,
                        moe,
                        use_moe_in_layer=moe_layer and moe.use_moe_ffn,
                    )
                )

        # å¦‚æœä½ ç”¨çš„æ˜¯ DictConfig
        # moe_cfg = MoECfg(**cfg.moe)
        # self.moe, self.n_local_experts = build_moe_from_cfg(moe_cfg)
        self.mask_layer_start = (
            mask_layer_start if mask_layer_start is not None else len(self.layers) - 1
        )
        self.classifier = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim * 2),  # å‡ç»´
            nn.GELU(),
            nn.LayerNorm(embedding_dim * 2),
            nn.Linear(embedding_dim * 2, embedding_dim),  # é™å›åŸç»´
            nn.GELU(),
            nn.LayerNorm(embedding_dim),
            nn.Linear(embedding_dim, num_bins + 1),  # è¾“å‡ºç±»åˆ«
        )

        # æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨å“ªç§regressor
        self.use_moe_regressor = use_moe_regressor
        if self.use_moe_regressor:
            self.regressor = MoERegressor(
                embedding_dim=embedding_dim,
                dropout=ffn_dropout,
                number_of_experts=number_of_experts,
                gate_temperature=1.0,
            )
        else:
            # åŸæ¥çš„simple regressor
            self.regressor = nn.Sequential(
                nn.Linear(embedding_dim, embedding_dim * 2),
                nn.ReLU(),
                nn.LayerNorm(embedding_dim * 2),
                nn.Dropout(0.1),
                nn.Linear(embedding_dim * 2, embedding_dim),
                nn.ReLU(),
                nn.LayerNorm(embedding_dim),
                nn.Dropout(0.1),
                nn.Linear(embedding_dim, embedding_dim // 2),
                nn.ReLU(),
                nn.LayerNorm(embedding_dim // 2),
                nn.Dropout(0.1),
                nn.Linear(embedding_dim // 2, 1),
            )

        self.enable_l0 = enable_l0
        self.enable_mse = enable_mse
        self.enable_ce = enable_ce
        # åˆå§‹åŒ– classifier å†…æ‰€æœ‰ Linear å±‚çš„æƒé‡å’Œåç½®
        for m in self.classifier:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
        self.fused_emb_proj = nn.Linear(2 * embedding_dim, embedding_dim)

    def forward(
        self,
        gene_ids,
        expression_bin,
        normalized_expr,
        return_encodings=False,
        return_mask_prob=True,
        return_gate_weights=False,
    ):
        """
        gene_ids: (batch, g)  # åŸºå› IDåºåˆ—
        expression_bin: (batch, g)  # ç¦»æ•£åŒ–çš„è¡¨è¾¾é‡
        normalized_expr: (batch, g)  # å½’ä¸€åŒ–çš„è¡¨è¾¾é‡
        return_gate_weights: æ˜¯å¦è¿”å›MoEçš„gateæƒé‡
        """
        gene_emb = self.gene_embedding(gene_ids)  # (batch, g, d)
        expr_emb = self.expr_embedding(expression_bin, normalized_expr)  # (batch, g, d)
        for i, layer in enumerate(self.layers):
            gene_emb, expr_emb = layer(gene_emb, expr_emb)
        for i, layer in enumerate(self.expression_layers):
            expr_emb = layer(expr_emb)
        final_emb = torch.cat([gene_emb, expr_emb], dim=-1)
        final_emb = self.fused_emb_proj(final_emb)  # (batch, g, d)
        if self.enable_mse and self.enable_ce:
            logits = self.classifier(final_emb)
            if return_gate_weights and self.use_moe_regressor:
                regression_output, gate_weights = self.get_regressor_output(final_emb)
                return logits, regression_output, gene_emb, expr_emb, gate_weights
            else:
                regression_output, _ = self.get_regressor_output(final_emb)
                return logits, regression_output, gene_emb, expr_emb
        elif self.enable_mse:
            if return_gate_weights and self.use_moe_regressor:
                regression_output, gate_weights = self.get_regressor_output(final_emb)
                return regression_output, gene_emb, expr_emb, gate_weights
            else:
                regression_output, _ = self.get_regressor_output(final_emb)
                return regression_output, gene_emb, expr_emb
        elif self.enable_ce:
            logits = self.classifier(final_emb)
            return logits, gene_emb, expr_emb

    def get_regressor_output(self, final_emb):
        """
        è·å–å›å½’å™¨è¾“å‡º

        Args:
            final_emb: æœ€ç»ˆèåˆçš„åµŒå…¥ç‰¹å¾

        Returns:
            regression_output: å›å½’é¢„æµ‹ç»“æœ
            gate_weights: MoEçš„gateæƒé‡ï¼ˆå¦‚æœä½¿ç”¨MoEï¼‰æˆ–Noneï¼ˆå¦‚æœä½¿ç”¨ç®€å•å›å½’å™¨ï¼‰
        """
        # æ ¹æ®ä½¿ç”¨çš„regressorç±»å‹è·å–è¾“å‡º
        if self.use_moe_regressor:
            # MoE regressorè¿”å›ä¸¤ä¸ªå€¼ï¼šregression_outputå’Œgate_weights
            regression_output, gate_weights = self.regressor(final_emb)
        else:
            # åŸæ¥çš„regressoråªè¿”å›ä¸€ä¸ªå€¼
            regression_output = self.regressor(final_emb)
            regression_output = regression_output.squeeze(-1)  # å»æ‰æœ€åä¸€ä¸ªç»´åº¦
            gate_weights = None  # åŸæ¥çš„regressoræ²¡æœ‰gate_weights

        return regression_output, gate_weights

    def get_all_moe_stats(self):
        """
        è·å–æ¨¡å‹ä¸­æ‰€æœ‰MoEå±‚çš„ç»Ÿè®¡ä¿¡æ¯

        Returns:
            dict: åŒ…å«æ‰€æœ‰MoEå±‚ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        moe_stats = {}

        # æ£€æŸ¥transformerå±‚ä¸­çš„MoE
        for i, layer in enumerate(self.layers):
            if hasattr(layer, "ffn_gene") and isinstance(layer.ffn_gene, MoE):
                moe_stats[f"transformer_layer_{i}_gene_ffn"] = (
                    layer.ffn_gene.get_expert_usage_stats()
                )
            if hasattr(layer, "ffn_expr") and isinstance(layer.ffn_expr, MoE):
                moe_stats[f"transformer_layer_{i}_expr_ffn"] = (
                    layer.ffn_expr.get_expert_usage_stats()
                )

        # æ£€æŸ¥expressionå±‚ä¸­çš„MoE
        for i, layer in enumerate(self.expression_layers):
            if hasattr(layer, "ffn_expr") and isinstance(layer.ffn_expr, MoE):
                moe_stats[f"expression_layer_{i}_expr_ffn"] = (
                    layer.ffn_expr.get_expert_usage_stats()
                )

        # æ£€æŸ¥MoE regressor
        if self.use_moe_regressor and hasattr(self.regressor, "gate"):
            moe_stats["moe_regressor"] = {
                "expert_usage_ratio": (
                    self.regressor.gate_weights_history
                    if hasattr(self.regressor, "gate_weights_history")
                    else "Not available"
                )
            }

        return moe_stats

    def check_moe_collapse(self, threshold=0.8):
        """
        æ£€æŸ¥æ¨¡å‹ä¸­æ˜¯å¦æœ‰MoEå±‚å‘ç”Ÿå¡Œç¼©

        Args:
            threshold: å¡Œç¼©é˜ˆå€¼

        Returns:
            dict: åŒ…å«å¡Œç¼©æ£€æµ‹ç»“æœçš„å­—å…¸
        """
        collapse_results = {}
        moe_stats = self.get_all_moe_stats()

        for layer_name, stats in moe_stats.items():
            if isinstance(stats, dict) and "collapse_ratio" in stats:
                is_collapsed = stats["collapse_ratio"] > threshold
                collapse_results[layer_name] = {
                    "is_collapsed": is_collapsed,
                    "collapse_ratio": stats["collapse_ratio"],
                    "entropy": stats["entropy"],
                    "expert_usage_ratio": (
                        stats["expert_usage_ratio"].tolist()
                        if hasattr(stats["expert_usage_ratio"], "tolist")
                        else stats["expert_usage_ratio"]
                    ),
                }

        return collapse_results

    def reset_all_moe_stats(self):
        """é‡ç½®æ‰€æœ‰MoEå±‚çš„ä½¿ç”¨ç»Ÿè®¡"""
        # é‡ç½®transformerå±‚ä¸­çš„MoE
        for layer in self.layers:
            if hasattr(layer, "ffn_gene") and isinstance(layer.ffn_gene, MoE):
                layer.ffn_gene.reset_usage_stats()
            if hasattr(layer, "ffn_expr") and isinstance(layer.ffn_expr, MoE):
                layer.ffn_expr.reset_usage_stats()

        # é‡ç½®expressionå±‚ä¸­çš„MoE
        for layer in self.expression_layers:
            if hasattr(layer, "ffn_expr") and isinstance(layer.ffn_expr, MoE):
                layer.ffn_expr.reset_usage_stats()

    def print_moe_collapse_report(self, threshold=0.8):
        """
        æ‰“å°MoEå¡Œç¼©æ£€æµ‹æŠ¥å‘Š

        Args:
            threshold: å¡Œç¼©é˜ˆå€¼
        """
        print(f"\n{'='*60}")
        print("MoE Collapse Detection Report")
        print(f"{'='*60}")
        print(f"Collapse Threshold: {threshold}")

        collapse_results = self.check_moe_collapse(threshold)

        if not collapse_results:
            print("No MoE layers found in the model.")
            return

        collapsed_layers = []
        healthy_layers = []

        for layer_name, result in collapse_results.items():
            if result["is_collapsed"]:
                collapsed_layers.append((layer_name, result))
            else:
                healthy_layers.append((layer_name, result))

        print(f"\nTotal MoE layers: {len(collapse_results)}")
        print(f"Collapsed layers: {len(collapsed_layers)}")
        print(f"Healthy layers: {len(healthy_layers)}")

        if collapsed_layers:
            print("\nğŸš¨ COLLAPSED LAYERS:")
            for layer_name, result in collapsed_layers:
                print(f"  - {layer_name}:")
                print(f"    Collapse Ratio: {result['collapse_ratio']:.4f}")
                print(f"    Entropy: {result['entropy']:.4f}")
                print(
                    f"    Expert Usage: {[f'{x:.3f}' for x in result['expert_usage_ratio']]}"
                )

        if healthy_layers:
            print("\nâœ… HEALTHY LAYERS:")
            for layer_name, result in healthy_layers:
                print(f"  - {layer_name}:")
                print(f"    Collapse Ratio: {result['collapse_ratio']:.4f}")
                print(f"    Entropy: {result['entropy']:.4f}")
                print(
                    f"    Expert Usage: {[f'{x:.3f}' for x in result['expert_usage_ratio']]}"
                )

        print(f"\n{'='*60}")

        # è¿”å›æ˜¯å¦æœ‰å¡Œç¼©
        return len(collapsed_layers) > 0

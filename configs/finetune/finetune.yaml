defaults:
  - model: deepsc
  - _self_

wandb_project: "DeepSC"
tags: ["256hd"]
run_name: "finetune_cell_type_annotation_256hd"

#文件加载相关
chunksize: 4
shuffel_files_each_epoch: False

pretrained_model_path: "/home/angli/DeepSC/results/pretraining_1201/DeepSC_11_0.ckpt"
load_pretrained_model: True

# Fine-tuning mode selection
# Options:
#   - "full": Full parameter fine-tuning (all model parameters)
#   - "head_only": Only fine-tune the classification head
finetune_mode: "full"
# Loss weights
enable_l0: False
enable_mse: True
enable_ce: True
ce_loss_weight: 1.0
target_mse_loss_weight: 3.0
#control the weight of mse loss
adaptive_mse_weight: False
cls_loss_type: "per_bin"

# MoE Regressor settings
use_moe_regressor: True  # 是否使用MoE回归器，False则使用原来的简单回归器
# True:  使用MoE回归器（3个专家网络：low/mid/high expert + gate网络）
# False: 使用原来的简单回归器（单一sequential网络）

# L0 loss warmup parameters
l0_warmup_start_epoch: 2  # 从第几个epoch开始加入l0 loss,并在一个epoch内线性增加到l0_lambda
enable_l0_warmup: False
l0_lambda: 0.1
# 数据增强相关
enable_data_augmentation: True  # 是否启用数据增强（动态掩码概率）

# 分类任务
# 最多只能打开一个开关
weighted_ce_loss: False
mean_ce_loss: True
use_ldam_loss: False
enable_adaptive_ce_loss: False
enable_alternating_ldam_mean_ce_loss: False
enable_warm_alternating_ldam_mean_ce_loss: False

# 回归任务
# 最多只能打开一个开关 选择huber还是mse
enable_mse_loss: False
enable_huber_loss: True
# 回归任务
# 最多只能打开一个开关 选择普通的还是加权的
use_normal_regression_loss: True
use_hard_regression_loss: False
use_exp_regression_loss: False

#Print details of mse loss in log
show_mse_loss_details: False
output_dir: "/home/angli/baseline/DeepSC/outputs/deepsc"
csv_path: "/home/angli/baseline/DeepSC-117-t86/scripts/data/preprocessing/gene_map_tp10k.csv"
seperated_train_eval_dataset: True
data_path: "/home/angli/DeepSC/data/processed/baseline/scgpt/myeloid_train.h5ad"
data_path_eval: "/home/angli/DeepSC/data/processed/baseline/scgpt/myeloid_test.h5ad"
var_name_in_h5ad: "gene_name"
obs_celltype_col: "celltype"
use_moe_ffn: False
data_length: 5000
seed: 42
ckpt_dir: "/home/angli/baseline/DeepSC/ckpts"
num_device: 1
num_nodes: 1  # 节点数，与SLURM --nodes配置匹配
valid_every: 200
model_name: "DeepSC_heart9_blood_others_blood"
grad_acc: 20
learning_rate: 3e-4
epoch: 10
batch_size: 32
num_bin: 5
save_ckpt_every: 200

log_on_wandb_every: 20
log_m_matrix_every: 1000
sequence_length: 1024
plot_tsne_and_umap: False
draw_continuous_pred_label_scatter: True

# Learning rate scheduler configuration
# Options:
#   - "constant": Constant learning rate (no scheduling)
#   - "cosine": Cosine annealing with warmup (default)
lr_scheduler_type: "constant"  # Change to "constant" for constant LR

# 新增warmup相关参数
use_warmup: False
warmup_ratio: 0.03
use_warmup_with_decay: False
use_scbert_scheduler: True
use_mogaide_scheduler: False

# 新增checkpoint相关参数
resume_last_training: True

# 评估策略
eval_use_all_common_types: true  # true: 显示所有共同类型(即使当前batch中为0), false: 只显示当前batch存在的类型

# 梯度检查相关参数
check_grad_flow: False  # 是否启用梯度检查 多卡的时候不需要启用
hydra:
  # Logging configuration: https://hydra.cc/docs/tutorials/basic/running_your_app/logging/
  verbose: False
  # 设置输出目录: 按日期/时间组织
  run:
    dir: /home/angli/DeepSC/results/cell_type_annotation/${now:%Y-%m-%d}/${now:%H-%M-%S}
  # Logs to stdout and to a file.
  job_logging:
    handlers:
      console:
        class: logging.StreamHandler
        stream: ext://sys.stdout
      file:
        filename:
          ${hydra.runtime.output_dir}/${hydra.job.name}_${oc.select:hydra.job.num, 0}.log

# Learning Rate Scheduler ä½¿ç”¨æŒ‡å—

**æ›´æ–°æ—¥æœŸ**: 2025-12-01

---

## æ¦‚è¿°

DeepSCç°åœ¨æ”¯æŒä¸¤ç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥:

1. **`constant`**: æ’å®šå­¦ä¹ ç‡,ä»å¤´åˆ°å°¾å­¦ä¹ ç‡ä¸å˜
2. **`cosine`**: Cosine annealing with warmup (é»˜è®¤)

---

## å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: ä¿®æ”¹ä¸»é…ç½®æ–‡ä»¶

ç¼–è¾‘ `configs/finetune/finetune.yaml`:

```yaml
# ä½¿ç”¨æ’å®šå­¦ä¹ ç‡
lr_scheduler_type: "constant"
learning_rate: 1e-4

# æˆ–ä½¿ç”¨cosine annealing (é»˜è®¤)
lr_scheduler_type: "cosine"
learning_rate: 1e-4
warmup_ratio: 0.03
```

### æ–¹æ³•2: å‘½ä»¤è¡Œè¦†ç›–

```bash
# ä½¿ç”¨æ’å®šå­¦ä¹ ç‡
python -m src.deepsc.finetune.finetune lr_scheduler_type=constant learning_rate=1e-4

# ä½¿ç”¨cosine annealing
python -m src.deepsc.finetune.finetune lr_scheduler_type=cosine learning_rate=1e-4
```

### æ–¹æ³•3: ä½¿ç”¨ç¤ºä¾‹é…ç½®æ–‡ä»¶

```bash
# hPancreas with constant LR
python -m src.deepsc.finetune.finetune --config-name=examples/constant_lr_hpancreas

# Myeloid with constant LR
python -m src.deepsc.finetune.finetune --config-name=examples/constant_lr_myeloid
```

---

## è¯¦ç»†é…ç½®è¯´æ˜

### 1. Constant Learning Rate (æ’å®šå­¦ä¹ ç‡)

**é…ç½®**:
```yaml
lr_scheduler_type: "constant"
learning_rate: 1e-4  # å­¦ä¹ ç‡ä¿æŒä¸å˜
```

**ç‰¹ç‚¹**:
- âœ… ç®€å•ç›´æ¥,æ— éœ€è°ƒå‚
- âœ… è®­ç»ƒç¨³å®š,å¯é¢„æµ‹
- âœ… é€‚åˆå°æ•°æ®é›†æˆ–ç®€å•ä»»åŠ¡
- âœ… ä¸éœ€è¦warmup
- âš ï¸ å¯èƒ½éœ€è¦æ›´å¤šepochs

**è¿è¡Œæ—¶è¾“å‡º**:
```
================================================================================
Using CONSTANT learning rate: 0.0001
No learning rate scheduling will be applied.
================================================================================
```

**æ¨èåœºæ™¯**:
- hPancreas (15ç±»,ç®€å•ä»»åŠ¡)
- å¿«é€Ÿå®éªŒå’ŒåŸå‹
- è®­ç»ƒä¸ç¨³å®šæ—¶å°è¯•

**æ¨èé…ç½®**:
```yaml
# ç®€å•æ•°æ®é›†
lr_scheduler_type: "constant"
learning_rate: 1e-4
epoch: 10

# å¤æ‚æ•°æ®é›†
lr_scheduler_type: "constant"
learning_rate: 5e-5  # æ›´ä½çš„LR
epoch: 20            # æ›´å¤šepochs
```

---

### 2. Cosine Annealing (ä½™å¼¦é€€ç«)

**é…ç½®**:
```yaml
lr_scheduler_type: "cosine"
learning_rate: 1e-4
warmup_ratio: 0.03  # 3% stepsç”¨äºwarmup
```

**ç‰¹ç‚¹**:
- ğŸ“ˆ å­¦ä¹ ç‡åŠ¨æ€å˜åŒ–
- ğŸ“ˆ åŒ…å«warmupå’Œcosine decay
- âœ… é€šå¸¸èƒ½è·å¾—æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½
- âš ï¸ éœ€è¦è°ƒæ•´warmup_ratio
- âš ï¸ å¯èƒ½åœ¨æŸäº›æ•°æ®é›†ä¸Šä¸ç¨³å®š

**è¿è¡Œæ—¶è¾“å‡º**:
```
================================================================================
Using COSINE ANNEALING with warmup:
  - Warmup steps: 150
  - Total steps: 5000
  - Initial LR: 0.0001
================================================================================
```

**å­¦ä¹ ç‡å˜åŒ–æ›²çº¿**:
```
LR
^
|     /\
|    /  \___
|   /       \___
|  /            \___
| /                 \___
|/________________________> Steps
  ^       ^              ^
  |       |              |
warmup  restart1      restart2
```

**æ¨èåœºæ™¯**:
- å¤§è§„æ¨¡æ•°æ®é›†
- é•¿æ—¶é—´è®­ç»ƒ(50+ epochs)
- è¿½æ±‚æœ€ä½³æ€§èƒ½

**æ¨èé…ç½®**:
```yaml
lr_scheduler_type: "cosine"
learning_rate: 1e-4
warmup_ratio: 0.03
epoch: 30
```

---

## ä¸åŒæ•°æ®é›†çš„æ¨èé…ç½®

### hPancreas (15ç±», 11,847ç»†èƒ)

#### é€‰é¡¹1: Constant LR (æ¨è)
```yaml
lr_scheduler_type: "constant"
learning_rate: 1e-4
epoch: 10
batch_size: 32
grad_acc: 20
```

**é¢„æœŸæ€§èƒ½**: 95-97% å‡†ç¡®ç‡

#### é€‰é¡¹2: Cosine Annealing
```yaml
lr_scheduler_type: "cosine"
learning_rate: 1e-4
warmup_ratio: 0.03
epoch: 10
batch_size: 32
grad_acc: 20
```

**é¢„æœŸæ€§èƒ½**: 96-98% å‡†ç¡®ç‡

---

### Myeloid (39ç±», 56,911ç»†èƒ)

#### é€‰é¡¹1: Constant LR (æ¨èç”¨äºç¨³å®šæ€§)
```yaml
lr_scheduler_type: "constant"
learning_rate: 5e-5  # æ›´ä½çš„LR
epoch: 20            # æ›´å¤šepochs
batch_size: 32
grad_acc: 20
```

**é¢„æœŸæ€§èƒ½**: 73-76% å‡†ç¡®ç‡
**ä¼˜ç‚¹**: è®­ç»ƒç¨³å®š,é¿å…æ³¢åŠ¨

#### é€‰é¡¹2: Cosine Annealing
```yaml
lr_scheduler_type: "cosine"
learning_rate: 1e-4
warmup_ratio: 0.05  # æ›´é•¿çš„warmup
epoch: 30
batch_size: 32
grad_acc: 20
```

**é¢„æœŸæ€§èƒ½**: 74-77% å‡†ç¡®ç‡
**æ³¨æ„**: å¯èƒ½å‡ºç°è®­ç»ƒä¸ç¨³å®š

---

### Zheng (11ç±», 52,748ç»†èƒ, é«˜ç¨€ç–åº¦)

#### æ¨è: Constant LR
```yaml
lr_scheduler_type: "constant"
learning_rate: 1e-4
epoch: 15
batch_size: 32
grad_acc: 20
```

**åŸå› **: é«˜ç¨€ç–åº¦(97%)éœ€è¦ç¨³å®šçš„å­¦ä¹ ç‡

---

### Segerstolpe (éœ€è¦å…ˆå½’ä¸€åŒ–!)

```yaml
# å¿…é¡»å…ˆå¯¹æ•°æ®è¿›è¡Œlog1på½’ä¸€åŒ–!
lr_scheduler_type: "constant"
learning_rate: 5e-5
epoch: 20
batch_size: 32
grad_acc: 20
```

---

## å­¦ä¹ ç‡é€‰æ‹©æŒ‡å—

### åŸºæœ¬åŸåˆ™

| æ•°æ®é›†å¤æ‚åº¦ | ç»†èƒç±»å‹æ•° | æ¨èLR (constant) | æ¨èLR (cosine) |
|------------|-----------|------------------|-----------------|
| ç®€å• | < 20 | 1e-4 | 1e-4 |
| ä¸­ç­‰ | 20-40 | 5e-5 | 1e-4 |
| å¤æ‚ | > 40 | 1e-5 | 5e-5 |

### è°ƒæ•´å»ºè®®

**å¦‚æœè®­ç»ƒlossä¸‹é™å¤ªæ…¢**:
- Constant LR: å¢å¤§learning_rate (å¦‚1e-4 â†’ 3e-4)
- Cosine: å¢å¤§learning_rateæˆ–å‡å°‘warmup_ratio

**å¦‚æœè®­ç»ƒä¸ç¨³å®š**:
- Constant LR: é™ä½learning_rate (å¦‚1e-4 â†’ 5e-5)
- Cosine: è€ƒè™‘åˆ‡æ¢åˆ°constant LR

**å¦‚æœéªŒè¯é›†æ€§èƒ½ä¸æå‡**:
- å¢åŠ epochs
- é™ä½learning_rate
- å°è¯•ä¸åŒçš„schedulerç±»å‹

---

## å®éªŒå¯¹æ¯”

### å®éªŒè®°å½•

| å®éªŒ | æ•°æ®é›† | Scheduler | LR | Epochs | æœ€ä½³å‡†ç¡®ç‡ | è®­ç»ƒç¨³å®šæ€§ |
|------|--------|-----------|-----|--------|-----------|-----------|
| 1 | hPancreas | constant | 1e-4 | 10 | 97.09% | â­â­â­â­â­ |
| 2 | hPancreas | cosine | 1e-4 | 10 | 97.30% | â­â­â­â­â­ |
| 3 | Myeloid | cosine | 1e-4 | 10 | 73.86% | â­â­â­ (ä¸ç¨³å®š) |
| 4 | Myeloid | constant | 5e-5 | 20 | ? | å¾…æµ‹è¯• |
| 5 | Zheng | cosine | 1e-4 | 10 | 79.04% | â­â­â­â­ |

**ç»“è®º**:
- hPancreas: ä¸¤ç§ç­–ç•¥éƒ½å¾ˆå¥½,cosineç•¥ä¼˜
- Myeloid: cosineä¸ç¨³å®š,å»ºè®®å°è¯•constant
- Zheng: cosineè¡¨ç°è‰¯å¥½

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜1: "Unknown lr_scheduler_type" é”™è¯¯

**åŸå› **: é…ç½®æ–‡ä»¶ä¸­lr_scheduler_typeå€¼æ— æ•ˆ

**è§£å†³**:
```yaml
# é”™è¯¯
lr_scheduler_type: "linear"  # âŒ ä¸æ”¯æŒ

# æ­£ç¡®
lr_scheduler_type: "constant"  # âœ…
# æˆ–
lr_scheduler_type: "cosine"    # âœ…
```

### é—®é¢˜2: Constant LRä½†å­¦ä¹ ç‡ä»åœ¨å˜åŒ–

**æ£€æŸ¥**: ç¡®è®¤è¿è¡Œæ—¥å¿—ä¸­æ˜¾ç¤º:
```
Using CONSTANT learning rate: 0.0001
No learning rate scheduling will be applied.
```

å¦‚æœçœ‹åˆ°cosineç›¸å…³ä¿¡æ¯,æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®åŠ è½½ã€‚

### é—®é¢˜3: è®­ç»ƒä¸ç¨³å®š

**å°è¯•**:
1. åˆ‡æ¢åˆ°constant LR
2. é™ä½learning_rate
3. å¢åŠ grad_acc (å‡å°‘æ›´æ–°é¢‘ç‡)
4. æ£€æŸ¥æ•°æ®æ˜¯å¦éœ€è¦å½’ä¸€åŒ–

---

## é«˜çº§æŠ€å·§

### 1. åŠ¨æ€åˆ‡æ¢ç­–ç•¥

å…ˆç”¨constant LRå¿«é€Ÿæ”¶æ•›,å†ç”¨cosineç²¾è°ƒ:

```bash
# é˜¶æ®µ1: å¿«é€Ÿæ”¶æ•› (epoch 1-10)
python -m src.deepsc.finetune.finetune \
    lr_scheduler_type=constant \
    learning_rate=1e-4 \
    epoch=10

# é˜¶æ®µ2: ç²¾è°ƒ (epoch 11-20)
python -m src.deepsc.finetune.finetune \
    lr_scheduler_type=cosine \
    learning_rate=5e-5 \
    epoch=20 \
    resume_last_training=True
```

### 2. å­¦ä¹ ç‡èŒƒå›´æµ‹è¯•

å¿«é€Ÿæµ‹è¯•ä¸åŒå­¦ä¹ ç‡:

```bash
for lr in 1e-5 5e-5 1e-4 5e-4; do
    python -m src.deepsc.finetune.finetune \
        lr_scheduler_type=constant \
        learning_rate=$lr \
        epoch=5 \
        run_name="lr_test_${lr}"
done
```

### 3. ç›‘æ§å­¦ä¹ ç‡

åœ¨è®­ç»ƒå¾ªç¯ä¸­,å­¦ä¹ ç‡ä¼šè¢«è‡ªåŠ¨è®°å½•åˆ°wandbã€‚
æŸ¥çœ‹ "learning_rate" æ›²çº¿æ¥éªŒè¯è°ƒåº¦å™¨æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

---

## æ€»ç»“

### å¿«é€Ÿå†³ç­–æ ‘

```
å¼€å§‹
  â”‚
  â”œâ”€ æ•°æ®é›†ç®€å• (< 20ç±»)?
  â”‚  â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ constant @ 1e-4
  â”‚  â””â”€ å¦ â†’ ç»§ç»­
  â”‚
  â”œâ”€ è®­ç»ƒç¨³å®šæ€§é‡è¦?
  â”‚  â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ constant @ 5e-5
  â”‚  â””â”€ å¦ â†’ ä½¿ç”¨ cosine @ 1e-4
  â”‚
  â””â”€ è®­ç»ƒæ—¶é—´å……è¶³ (>20 epochs)?
     â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ cosine @ 1e-4
     â””â”€ å¦ â†’ ä½¿ç”¨ constant @ 1e-4
```

### é»˜è®¤æ¨è

**åˆæ¬¡å°è¯•**: ä½¿ç”¨ `constant @ 1e-4`
- ç®€å•
- ç¨³å®š
- æ˜“äºè°ƒè¯•

**è¿½æ±‚æ€§èƒ½**: ä½¿ç”¨ `cosine @ 1e-4`
- å¯èƒ½è·å¾—æ›´å¥½çš„ç»“æœ
- éœ€è¦æ›´å¤šè°ƒå‚

---

## å‚è€ƒèµ„æ–™

- é…ç½®æ–‡ä»¶: `configs/finetune/finetune.yaml`
- ç¤ºä¾‹é…ç½®: `configs/finetune/examples/`
- æºä»£ç : `src/deepsc/finetune/cell_type_annotation.py` (ç¬¬199-254è¡Œ)

---

**æœ€åæ›´æ–°**: 2025-12-01
**ç»´æŠ¤è€…**: DeepSC Team

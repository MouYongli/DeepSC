#!/usr/bin/env python3
"""
æ›´æ–°åçš„wandb resumeåŠŸèƒ½æµ‹è¯•è„šæœ¬
æ–°é€»è¾‘ï¼šä¸åˆ›å»ºç©ºçš„runï¼Œç›´æ¥ç”¨æ­£ç¡®çš„run_idåˆå§‹åŒ–
"""
import os

import torch

import sys
import wandb

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

from src.deepsc.utils.utils import save_checkpoint


def test_wandb_resume_optimized():
    """æµ‹è¯•ä¼˜åŒ–åçš„ wandb resume åŠŸèƒ½ - ä¸åˆ›å»ºç©ºrun"""

    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = "./test_checkpoints"
    os.makedirs(test_dir, exist_ok=True)

    print("=== æµ‹è¯•ä¼˜åŒ–åçš„ wandb resume åŠŸèƒ½ ===")

    # 1. æ¨¡æ‹Ÿç¬¬ä¸€æ¬¡è®­ç»ƒ
    print("\n1. æ¨¡æ‹Ÿç¬¬ä¸€æ¬¡è®­ç»ƒ...")

    # åˆå§‹åŒ–wandb
    wandb.init(
        project="test_deepsc_resume",
        name="test_run_original",
        tags=["test", "first_run"],
        config={"test": True, "run_number": 1},
    )

    original_run_id = wandb.run.id
    original_name = wandb.run.name
    original_project = wandb.run.project
    original_entity = wandb.run.entity
    original_tags = list(wandb.run.tags)
    original_config = dict(wandb.run.config)

    print(f"åŸå§‹run_id: {original_run_id}")
    print(f"åŸå§‹name: {original_name}")
    print(f"åŸå§‹tags: {original_tags}")

    # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = torch.nn.Linear(10, 1)
    optimizer = torch.optim.Adam(model.parameters())
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)

    # è®°å½•ä¸€äº›æ•°æ®
    wandb.log({"epoch": 1, "loss": 0.5})
    wandb.log({"epoch": 2, "loss": 0.3})

    # ä¿å­˜checkpointï¼ˆåŒ…å«wandbé…ç½®ï¼‰
    save_checkpoint(
        epoch=5,
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
        model_name="test_model",
        ckpt_folder=test_dir,
        iteration=100,
    )

    # ç»“æŸç¬¬ä¸€æ¬¡wandbä¼šè¯
    wandb.finish()

    # 2. æ¨¡æ‹Ÿç¬¬äºŒæ¬¡è®­ç»ƒï¼ˆæ¢å¤åœºæ™¯ï¼‰
    print("\n2. æ¨¡æ‹Ÿè®­ç»ƒæ¢å¤...")

    # æ¨¡æ‹Ÿåœ¨trainerä¸­çš„é€»è¾‘ï¼šå…ˆå°è¯•åŠ è½½checkpoint
    ckpt_path = os.path.join(test_dir, "latest_checkpoint.pth")
    if os.path.exists(ckpt_path):
        print("å‘ç°checkpointï¼Œå°è¯•åŠ è½½...")

        # åŠ è½½checkpoint
        checkpoint = torch.load(ckpt_path, map_location="cpu")
        saved_run_id = checkpoint.get("wandb_run_id", None)
        saved_wandb_config = checkpoint.get("wandb_config", None)

        if saved_run_id and saved_wandb_config:
            print(f"æ‰¾åˆ°ä¿å­˜çš„wandb run_id: {saved_run_id}")
            print("ä½¿ç”¨åŸå§‹é…ç½®æ¢å¤wandbä¼šè¯...")

            # ç›´æ¥ç”¨åŸå§‹run_idå’Œé…ç½®åˆå§‹åŒ–wandb
            wandb.init(
                id=saved_run_id,
                resume="allow",
                project=saved_wandb_config.get("project"),
                entity=saved_wandb_config.get("entity"),
                name=saved_wandb_config.get("name"),
                tags=saved_wandb_config.get("tags"),
                config=saved_wandb_config.get("config"),
            )

            resumed_run_id = wandb.run.id
            resumed_name = wandb.run.name
            resumed_tags = list(wandb.run.tags)

            print(f"æ¢å¤åçš„run_id: {resumed_run_id}")
            print(f"æ¢å¤åçš„name: {resumed_name}")
            print(f"æ¢å¤åçš„tags: {resumed_tags}")

            # éªŒè¯æ¢å¤æ˜¯å¦æˆåŠŸ
            if (
                resumed_run_id == original_run_id
                and resumed_name == original_name
                and resumed_tags == original_tags
            ):
                print("âœ… wandb æ¢å¤æˆåŠŸï¼ä½¿ç”¨äº†åŸå§‹çš„run_idå’Œé…ç½®")

                # ç»§ç»­è®°å½•æ•°æ®
                wandb.log({"epoch": 6, "loss": 0.2})
                wandb.log({"epoch": 7, "loss": 0.1})

                # ä¿å­˜å¦ä¸€ä¸ªcheckpoint
                save_checkpoint(
                    epoch=7,
                    model=model,
                    optimizer=optimizer,
                    scheduler=scheduler,
                    model_name="test_model",
                    ckpt_folder=test_dir,
                    iteration=140,
                )

                wandb.finish()

                print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
                return True
            else:
                print("âŒ æ¢å¤çš„é…ç½®ä¸åŒ¹é…")
                return False
        else:
            print("âŒ checkpoint ä¸­æ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„ wandb é…ç½®")
            return False
    else:
        print("âŒ checkpoint æ–‡ä»¶ä¸å­˜åœ¨")
        return False

    # 3. æ¸…ç†æµ‹è¯•æ–‡ä»¶
    print("\n3. æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
    import shutil

    shutil.rmtree(test_dir)


def test_no_checkpoint_scenario():
    """æµ‹è¯•æ²¡æœ‰checkpointçš„æƒ…å†µ"""

    print("\n=== æµ‹è¯•æ²¡æœ‰checkpointçš„æƒ…å†µ ===")

    # æ¨¡æ‹Ÿtraineråœ¨æ²¡æœ‰checkpointæ—¶çš„è¡Œä¸º
    print("æ²¡æœ‰checkpointï¼Œåˆ›å»ºæ–°çš„wandb run...")

    wandb.init(
        project="test_deepsc_no_checkpoint",
        name="test_new_run",
        tags=["test", "new_run"],
        config={"test": True, "is_new": True},
    )

    new_run_id = wandb.run.id
    print(f"æ–°çš„run_id: {new_run_id}")

    # è®°å½•ä¸€äº›æ•°æ®
    wandb.log({"epoch": 1, "loss": 1.0})

    wandb.finish()

    print("âœ… æ–°runåˆ›å»ºæˆåŠŸ")
    return True


if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•ä¼˜åŒ–åçš„ wandb resume åŠŸèƒ½...")

    # æµ‹è¯•æ²¡æœ‰checkpointçš„æƒ…å†µ
    test1_success = test_no_checkpoint_scenario()

    # æµ‹è¯•æœ‰checkpointæ¢å¤çš„æƒ…å†µ
    test2_success = test_wandb_resume_optimized()

    if test1_success and test2_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… ä¼˜åŒ–åçš„wandb resumeåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("âœ… ä¸ä¼šåˆ›å»ºç©ºçš„wandb run")
        print("âœ… æ¢å¤æ—¶ä½¿ç”¨åŸå§‹çš„runé…ç½®")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼")

    print("\næ³¨æ„ï¼šæ–°çš„é€»è¾‘ç¡®ä¿äº†ï¼š")
    print("1. å¦‚æœæœ‰checkpointï¼Œç›´æ¥ç”¨åŸå§‹run_idåˆå§‹åŒ–wandb")
    print("2. å¦‚æœæ²¡æœ‰checkpointï¼Œåˆ›å»ºæ–°çš„wandb run")
    print("3. ä¸ä¼šåœ¨wandbç½‘ç«™ä¸Šç•™ä¸‹ç©ºçš„run")

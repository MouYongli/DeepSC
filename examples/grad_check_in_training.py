#!/usr/bin/env python3
"""
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥çš„ç¤ºä¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim

from deepsc.utils.utils import check_grad_flow


class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(10, 20)
        self.layer2 = nn.Linear(20, 10)
        self.layer3 = nn.Linear(10, 5)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)
        return x


def train_with_grad_check():
    """åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¹¶ç›‘æ§æ¢¯åº¦...")

    # åˆ›å»ºæ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®
    model = SimpleModel()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    x = torch.randn(100, 10)
    target = torch.randint(0, 5, (100,))

    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters())}")
    print(
        f"å¯è®­ç»ƒå‚æ•°æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad)}"
    )

    # è®­ç»ƒå¾ªç¯
    for epoch in range(3):
        print(f"\nğŸ“Š Epoch {epoch + 1}")

        for step in range(10):  # æ¯ä¸ªepoch 10æ­¥
            # å‰å‘ä¼ æ’­
            output = model(x)
            loss = criterion(output, target)

            # æ¢¯åº¦æ£€æŸ¥ï¼ˆæ¯5æ­¥æ£€æŸ¥ä¸€æ¬¡ï¼‰
            if step % 5 == 0:
                print(f"\nğŸ” [æ¢¯åº¦æ£€æŸ¥] Epoch {epoch + 1}, Step {step}")
                grad_stats = check_grad_flow(
                    model, loss, verbose=False, retain_graph=True
                )

                # åˆ†ææ¢¯åº¦çŠ¶æ€
                total_params = (
                    len(grad_stats["ok"])
                    + len(grad_stats["zero"])
                    + len(grad_stats["none"])
                )
                ok_ratio = (
                    len(grad_stats["ok"]) / total_params if total_params > 0 else 0
                )

                print(
                    f"ğŸ“ˆ æ¢¯åº¦å¥åº·åº¦: {ok_ratio:.2%} ({len(grad_stats['ok'])}/{total_params})"
                )

                # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
                if len(grad_stats["zero"]) > len(grad_stats["ok"]):
                    print("âš ï¸ è­¦å‘Š: æ¢¯åº¦æ¶ˆå¤±å¯èƒ½æ­£åœ¨å‘ç”Ÿ")
                if len(grad_stats["none"]) > 0:
                    print("âŒ è­¦å‘Š: å‘ç°å†»ç»“çš„å‚æ•°")

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            if step % 5 == 0:
                print(f"Loss: {loss.item():.4f}")

    print("\nâœ… è®­ç»ƒå®Œæˆ!")


def test_gradient_vanishing():
    """æµ‹è¯•æ¢¯åº¦æ¶ˆå¤±çš„æƒ…å†µ"""
    print("\nğŸ§ª æµ‹è¯•æ¢¯åº¦æ¶ˆå¤±æƒ…å†µ...")

    # åˆ›å»ºä¸€ä¸ªå®¹æ˜“å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±çš„æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(10, 5),
        nn.Sigmoid(),  # Sigmoidå®¹æ˜“å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
        nn.Linear(5, 3),
        nn.Sigmoid(),
        nn.Linear(3, 2),
        nn.Sigmoid(),
        nn.Linear(2, 1),
        nn.Sigmoid(),
    )

    x = torch.randn(10, 10)
    target = torch.randn(10, 1)
    criterion = nn.MSELoss()

    # å‰å‘ä¼ æ’­
    output = model(x)
    loss = criterion(output, target)

    print("ğŸ” æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±æƒ…å†µ...")
    grad_stats = check_grad_flow(model, loss, verbose=True, retain_graph=False)

    # åˆ†æç»“æœ
    total_params = (
        len(grad_stats["ok"]) + len(grad_stats["zero"]) + len(grad_stats["none"])
    )
    if len(grad_stats["zero"]) > len(grad_stats["ok"]):
        print("âš ï¸ æ£€æµ‹åˆ°æ¢¯åº¦æ¶ˆå¤±é—®é¢˜!")
        print("ğŸ’¡ å»ºè®®: ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡")
    else:
        print("âœ… æ¢¯åº¦ä¼ å¯¼æ­£å¸¸")


if __name__ == "__main__":
    # è¿è¡ŒåŸºæœ¬è®­ç»ƒç¤ºä¾‹
    train_with_grad_check()

    # è¿è¡Œæ¢¯åº¦æ¶ˆå¤±æµ‹è¯•
    test_gradient_vanishing()
